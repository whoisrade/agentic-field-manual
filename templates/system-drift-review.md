# System Drift Review

Run quarterly, or after any major incident or release.

---

## Date & Attendees

| Field | Value |
|-------|-------|
| **Date** | |
| **Facilitator** | |
| **Attendees** | |
| **Last review date** | |

---

## Drift Signals

Check all that apply:

- [ ] Output quality variance has increased
- [ ] Recompute rate is rising
- [ ] Cost per outcome is rising (traffic flat)
- [ ] Traceability gaps are growing
- [ ] Latency is changing user behavior
- [ ] Safety escalations are rising
- [ ] Manual overrides are increasing
- [ ] Retry rates are climbing
- [ ] Cache hit rates are dropping

**Notes on signals observed:**


---

## Root Cause Investigation

### What changed in the state model?


### What changed in interaction contracts (UX)?


### What changed in orchestration?


### What changed in policy or model versions?


### What changed in tool reliability?


### What changed in traffic patterns?


---

## Impact Assessment

| Dimension | Impact (1-5) | Notes |
|-----------|-------------|-------|
| Cost | | |
| Quality | | |
| Latency | | |
| Compliance | | |
| User satisfaction | | |

---

## Remediation Plan

### Actions to restore legibility


### Actions to reduce recompute


### Actions to close auditability gaps


### Actions to restore safety/compliance boundaries


---

## Owners & Timelines

| Remediation Item | Owner | Deadline | Evidence of Completion |
|-----------------|-------|----------|----------------------|
| | | | |
| | | | |
| | | | |
| | | | |

---

## Follow-Up

**Next review date:**

**Escalation if not resolved by deadline:**

---

## The Litmus Test

> If you cannot assign owners and timelines to drift fixes within the review, the system will keep drifting.

---

*Review completed by:*
*Date:*
